# Cognizant: NLP model for customer intent classification üè¶üí°

## üöÄ Project Overview
**Cognizant** is a cutting-edge NLP model designed to classify and prioritize customer service queries by intent and severity. Built with TensorFlow, this project showcases a machine learning notebook that demonstrates a complete pipeline from data cleaning to model deployment.

## üìò Project Description
At the heart of **Cognizant** is the drive to understand and serve the customer better. This project encapsulates the journey from raw data to actionable insights, enabling businesses to anticipate needs and tailor their responses with unprecedented accuracy. Through the lens of this model, customer communication transforms into a powerful tool for satisfaction and engagement.


## ‚ú® Features
- **Exploratory Data Analysis (EDA)**: Dive into customer queries to unveil insights.
- **Text Preprocessing**: Elevate raw text to a structured symphony ready for deep learning applications.
- **Model Training**: Construct and train a robust neural network with TensorFlow.
- **Model Evaluation**: Assess performance with precision.
- **Severity Categorization**: Automate urgency assessment to elevate customer service.

## üíª Technologies Used

| Core Languages & Frameworks | Data Processing & Visualization |
| --------------------------- | ------------------------------- |
| **Python** - Foundation code. | **Pandas** - For efficient data manipulation and analysis. |
| **TensorFlow** - Framework for deep learning models. | **NumPy** - For numerical computations. |
| **Keras** - For TensorFlow's capabilities. | **Matplotlib & Seaborn** - For creating insightful visualizations. |
| **Jupyter Notebook** - For live coding. | **Scikit-learn** - For various machine learning tasks. |


## üõ† Usage
1. Clone the repository to your local machine:
    ```bash
    git clone https://github.com/MeftohuAhmed/Cognizant.git
    cd Cognizant
    ```

2. Install the necessary libraries

3. Open the notebook:
    ```bash
    jupyter notebook Notebook.ipynb
    ```

4. Inside the Jupyter environment, run each cell to witness the model's learning journey.



## üîë Key Learnings

- **Empathy Through Algorithms**: Fine-tuned NLP to discern not just the 'what,' but the 'why' behind customer queries, enhancing the human element in digital support.
- **Data as a Storyteller**: Transformed raw data into a narrative that speaks volumes about user needs, emphasizing the importance of context.
- **Elegance in Simplicity**: Struck a delicate balance between model sophistication and operational efficiency, debunking the myth that complexity equals capability.
- **Metrics with Meaning**: Selected metrics that resonate with real-world impact, aligning model performance with business objectives.
- **Urgency Understanding**: Elevated the prioritization process by integrating severity analysis, directly influencing customer trust and loyalty.
- **The Iterative Imperative**: Embraced the iterative nature of ML, where continuous refinement is not just a strategy, but a philosophy for growth and learning.



<details>
  <summary>üé¨ Demo</summary>

  ![Demo](https://github.com/MeftohuAhmed/Cognizant/assets/91487090/7ca8012a-ca5e-4685-ae00-0d3f58d73277)

</details>


<details>
  <summary>:chart_with_downwards_trend:Model Loss Progression</summary>

  ![Training and Validation Loss](https://github.com/MeftohuAhmed/Cognizant/blob/48e46f909a33c7cb0026fbadc25c253fe99ba413/Training%20Vs%20Validation.png)

  This chart captures the journey of the NLP model's learning process over successive training epochs. At the outset, we see a sharp decrease in training loss, indicating a quick uptake in learning from the training data. As training progresses, the curve flattens, suggesting that the model is starting to solidify its understanding and approaching an optimal state.

  Concurrently, the validation loss descends along a similar trajectory, affirming that our model is not just memorizing the data but is genuinely developing its predictive capabilities. The steady behavior of the validation loss, closely shadowing the training loss, gives us confidence in the model's ability to generalize to new, unseen data.

  The close proximity of the two curves throughout the training indicates a harmonious balance between learning and generalization, which is exactly what we aim for in a robust predictive model. The graph stands as a testament to the thoughtful architecture and fine-tuning that went into the model's development.

</details>
